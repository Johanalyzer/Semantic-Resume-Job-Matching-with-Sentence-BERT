{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Semantic Resume–Job Matching Using Fine-Tuned Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Project Documentation\n",
    "\n",
    "### Team Members\n",
    "- Maxwell Bernard & Johan Schommartz\n",
    "\n",
    "### Central Problem, Domain, and Data Characteristics\n",
    "\n",
    "**Problem:**  \n",
    "We built a semantic similarity model that predicts how well a candidate’s CV matches a job description focusing on the tech industry. Future work would integrate this into a job recommendation system that ranks job postings for a given CV.\n",
    "\n",
    "**Domain:**  \n",
    "Semantic matching, embeddings, psuedo-labeling, resume screening, job recommendation.\n",
    "\n",
    "**Data:** \n",
    "- Two Kaggle datasets:\n",
    "  - [Public resume dataset](https://www.kaggle.com/datasets/suriyaganesh/resume-dataset-structured) (50,000+ CVs with skills, job titles, experience)\n",
    "  - [Scraped LinkedIn Job Postings](https://www.kaggle.com/datasets/asaniczka/1-3m-linkedin-jobs-and-skills-2024) (1.3m+ jobs with descriptions, required skills)\n",
    "- Dataset cleaned, normalized and balanced:\n",
    "  - Removed low-information CVs (e.g. fewer than 4 skills)\n",
    "  - Filtered to tech-related job titles\n",
    "  - Normlaized skills and job titles using regex library\n",
    "  - Cleaned job titles and skills\n",
    "  - Balanced to 25,000 samples per class \n",
    "- Paired CV text and job description text, each labeled as:\n",
    "  - 1 → relevant match\n",
    "  - 0 → non-match\n",
    "- Train/val split with stratification for class balance\n",
    "\n",
    "### Model Architecture and Training\n",
    "\n",
    "**Model Architecture:**  \n",
    "- Sentence-BERT (SBERT) bi-encoder (`sentence-transformers/msmarco-bert-base-dot-v5`) was used to generate independent embeddings for CV text and job description text.  \n",
    "- These embeddings were compared using cosine similarity to estimate match relevance, and pseudo-labels were generated by combining similarity scores with heuristics of skill overlap and job-title alignment.  \n",
    "- During fine-tuning, we updated the full SBERT model (no layers were frozen), allowing the transformer to adjust its internal representation space toward CV–job alignment.\n",
    "- We trained the model using `CosineSimilarityLoss`, which directly optimizes the cosine distance between positive and negative CV–job pairs in the embedding space.\n",
    "- We did not freeze any layers because SBERT is pretrained for general semantic similarity, not for the CV–job domain. Fine-tuning the entire model allows all transformer layers to adapt to domain-specific patterns (skills and job titles), which leads to a better embedding space for our task. Freezing layers is mainly useful for small or closely related task.\n",
    "\n",
    "\n",
    "**Why This Architecture:**  \n",
    "- SBERT is purpose-built for semantic similarity and sentence-level embeddings.  \n",
    "- The bi-encoder setup allows independent CV and job embeddings, enabling fast retrieval and scalable matching.  \n",
    "- It consistently outperforms baseline SBERT on embedding-based tasks due to its contrastive pretraining strategy.\n",
    "\n",
    "\n",
    "**Training Mechanisms:**  \n",
    "- Token length of training documents truncated to `max_seq_length = 256` to:\n",
    "  - Reduce CPU bottleneck when tokenizing long CVs and job skills.\n",
    "  - This token length accommodates most CVs while retaining key information.\n",
    "  - Prevent memory issues from 1000+ token resumes.\n",
    "- Batch size was set to **16**, which provided the best trade-off between:\n",
    "  - Memory efficiency (larger batches caused out-of-memory issues due to long sequences)\n",
    "  - Stable gradient updates for contrastive learning\n",
    "  - Reasonable training speed on Kaggle hardware.\n",
    "\n",
    "- Only **1 epoch** of fine-tuning was used because:\n",
    "  - The dataset is large and already balanced (50k pairs total).\n",
    "  - SBERT is pretrained for semantic similarity, so it adapts quickly.\n",
    "  - Additional epochs gave diminishing returns while increasing training time.\n",
    "\n",
    "- `warmup_steps` was set to **10% of the dataloader size**, following Sentence-Transformers best practices:\n",
    "  - This gradually increases the learning rate at the start.\n",
    "  - Prevents unstable early updates, especially with contrastive objectives.\n",
    "  - Helps the model stabilise before reaching full learning rate.\n",
    "\n",
    "- `collate_fn = ft_model.smart_batching_collate` was used because:\n",
    "  - Smart batching groups samples by similar sequence lengths.\n",
    "  - This minimizes padding and speeds up training.\n",
    "  - It improves memory efficiency when working with long, uneven CV and job texts.\n",
    "\n",
    "### Key Experiments and Results\n",
    "\n",
    "#### Baseline (no fine-tuning)\n",
    "Metrics:\n",
    "- ROC–AUC: 0.9988 \n",
    "- Accuracy: 0.4834 \n",
    "\n",
    "Score distributions:\n",
    "  - Positives: mean ≈ 0.95\n",
    "  - Negatives: mean ≈ 0.89\n",
    "  - Margin: 0.0627\n",
    "\n",
    "Baseline SBERT ranked positive pairs slightly above negatives (yielding a very high ROC–AUC), but the cosine similarity distributions overlapped heavily. Because the margin between classes was tiny, no threshold could cleanly separate matches from non-matches, resulting in very low accuracy.\n",
    "\n",
    "Baseline SBERT understands language well, but lacks resume/job domain alignment.\n",
    "\n",
    "#### Fine-tuned SBERT\n",
    "Metrics:\n",
    "- ROC–AUC: 0.9993\n",
    "- Accuracy: 0.9893 \n",
    "\n",
    "Score distributions:\n",
    "  - Positives: mean ≈ 0.93\n",
    "  - Negatives: mean ≈ 0.03\n",
    "  - Margin: 0.9039\n",
    "\n",
    "Fine-tuning widened the separation between positive and negative examples. Positive pairs were pushed toward cosine ≈ 1.0 and negatives toward ≈ 0.0, making the classes easily separable. This resulted in a very high accuracy of 98.93% while maintaining a high ROC–AUC.\n",
    "\n",
    "Fine-tuning significantly improves the model’s ability to capture CV–job relevance.\n",
    "\n",
    "#### Embedding Visualisation\n",
    "- The Distribution of Cosine Similarity Scores visual shows:\n",
    "  - Before fine-tuning, positive and negative embedding scores tightly clustered and overlap heavily.\n",
    "  - After fine-tuning, the distributions separate cleanly with minimal overlap, reflecting the much larger similarity margin.\n",
    "\n",
    "## Discussion: Summary and Lessons Learned\n",
    "\n",
    "### What Worked Well\n",
    "- Fine-tuning SBERT provided large performance improvements  \n",
    "- Data balancing prevented model from collapsing to majority class  \n",
    "- 256-token truncation was the optimal trade-off between speed and accuracy  \n",
    "\n",
    "### What Could Be Improved\n",
    "- We could expand beyond tech-focused roles by incorporating CVs and job descriptions from other domains (e.g., healthcare, finance, education), which would make the model more generalisable and better at understanding a wider variety of career paths.\n",
    "  \n",
    "- Although we already performed basic normalisation (mapping acronyms like “js” → “javascript” and cleaning job-title variants), we could push this further by using a full standard classification for skills and titles and applying more aggressive variant merging. This would unify noisy data under consistent labels and produce cleaner, more reliable model inputs.\n",
    "  \n",
    "- We could apply hard-negative mining by adding non-matching CV–job pairs that appear highly similar (e.g., “Data Analyst” vs. “Business Analyst”), which would encourage the model to learn subtle semantic differences. This would make it easier for the model to clearly separate good matches from bad ones and reduce false-positive rates.\n",
    "\n",
    "\n",
    "### Most Important Takeaways\n",
    "- Preprocessing was the biggest challenge: Cleaning noisy, scraped CV and job data had more impact on performance than any modelling decision, highlighting the importance of well-structured inputs.\n",
    "  \n",
    "- Bi-encoder architectures work extremely well for CV–job matching: They provide scalable, independent embeddings and benefit strongly from domain-specific fine-tuning.\n",
    "- Sequence length control is crucial: Truncation and batching strategies directly influenced training stability, speed, and memory usage.\n",
    "  \n",
    "- Fine-tuning reshapes the embedding space: Even one epoch of contrastive learning dramatically improved class separability, demonstrating the value of task-specific adaptation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Theoretical Foundation\n",
    "\n",
    "Our approach is inspired by **ConFit** (Yun et al., 2024), which proposes a **BERT bi-encoder** trained with **contrastive learning** for semantic matching tasks.\n",
    "\n",
    "### ConFit: BERT Bi-Encoder with Contrastive Loss\n",
    "\n",
    "- **Bi-encoder architecture**  \n",
    "  - A *single* BERT encoder maps each input text (here: CV or job posting) to a **dense embedding**.  \n",
    "  - Semantic similarity between two texts is measured via **cosine similarity** of their embeddings.\n",
    "\n",
    "- **Contrastive learning objective**  \n",
    "  - The model is trained to **pull together** embeddings of **positive pairs** (texts that should match) and **push apart** embeddings of **negative pairs**.  \n",
    "  - ConFit uses a **contrastive loss** (InfoNCE-style) where:\n",
    "    - For each anchor text, the corresponding positive example should have higher similarity than the negatives in the batch.\n",
    "    - All other examples in the batch act as **in-batch negatives**, making training efficient.\n",
    "\n",
    "- **Pseudo-labelling in ConFit**  \n",
    "  - ConFit does not rely solely on manually labelled pairs.  \n",
    "  - Instead, it uses a base model to retrieve candidate pairs and applies **pseudo-labelling**:  \n",
    "    - High-confidence pairs from retrieval are treated as **positives**.  \n",
    "    - Low-confidence or mismatched pairs are treated as **negatives**.  \n",
    "  - This allows scaling to many training pairs without expensive human annotation.\n",
    "\n",
    "### Heuristic Pseudo-Labelling\n",
    "\n",
    "We follow the **architecture and training philosophy** of ConFit (BERT bi-encoder + contrastive loss), but build our own **pseudo-labelling pipeline**:\n",
    "\n",
    "- Inspired by **Vanetik & Kogan (2023)**, we use **word-level matching** as a cheap but effective signal of semantic relatedness:\n",
    "  - We represent each CV and job posting as **sets of cleaned skill tokens**.\n",
    "  - We also compare simplified **job titles** (e.g. “Senior Data Engineer” → `{data, engineer}`).\n",
    "- For each CV, we:\n",
    "  - Retrieve candidate job postings.\n",
    "  - Compute **skill overlap** and **title overlap**.\n",
    "  - Label:\n",
    "    - Pairs with **strong overlap** as **pseudo-positives**.\n",
    "    - Pairs with **no overlap** (within a candidate pool) as **pseudo-negatives**.\n",
    "\n",
    "The **fine-tuning strategy** (bi-encoder SBERT + contrastive loss over positive and negative pairs) follows ConFit.  \n",
    "The **word-overlap heuristic** for pseudo-labelling follows the spirit of Vanetik & Kogan’s word-level matching, adapted to our CV–job setting.\n",
    "\n",
    "### Training Strategy References\n",
    "\n",
    "**Batch size of 16:**\n",
    "- The Reimers & Gurevych (2019) Sentence-BERT paper emphasises small batch sizes for contrastive learning due to memory constraints of transformer encoders.\n",
    "- Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. ACL. https://arxiv.org/abs/1908.10084\n",
    "\n",
    "**Single epoch training:**\n",
    "- Reimers & Gurevych (2019) show effective adaptation after short fine-tuning, so we chose 1 epoch as the best trade-off between performance and training time.\n",
    "\n",
    "**Warmup steps**\n",
    "- We set warmup_steps to 10% of the total training steps because this is the recommended proportion in the official Sentence-Transformers fit() documentation and is widely used to stabilise early updates when fine-tuning pretrained transformer models.\n",
    "- The warmup phase gradually increases the learning rate from 0 to its maximum before linearly decaying, preventing sudden large gradient updates at the start of training, which is important when working with contrastive objectives and long sequences like CVs.\n",
    "- Sentence-Transformers documentation: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.fit \n",
    "\n",
    "**Smart batching:**\n",
    "- we used the built-in `smart_batching_collate` function from Sentence-Transformers to group samples of similar lengths together in each batch. This minimizes padding and speeds up training, improving memory efficiency when working with long, uneven CV and job texts.\n",
    "- This approach was inspired by Reimers & Gurevych (2019) who implemented this smart batching strategy in their Sentence-BERT framework to reduce computational overhead from padding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Mini-Project Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "\n",
    "To keep the workflow clear, we structure the notebook into the following steps:\n",
    "\n",
    "1. **Import libraries & set paths**  \n",
    "2. **Resume data preparation**\n",
    "   - Load and aggregate experience and skills.\n",
    "   - Clean and filter the skills field.\n",
    "   - Remove low-information and duplicate CVs.\n",
    "3. **Job posting data preparation**\n",
    "   - Load LinkedIn job data and job–skill mappings.\n",
    "   - Filter to relevant rows and build structured job representations.\n",
    "4. **Label-document creation**\n",
    "   - Build compact text inputs (“label documents”) for CVs and jobs.\n",
    "5. **Pseudo-label generation**\n",
    "   - Construct CV–job pairs using skill/title overlap.\n",
    "   - Balance positive and negative pairs and split into train/validation.\n",
    "6. **Model fine-tuning and baseline comparison**\n",
    "   - Baseline SBERT: frozen encoder + cosine similarity.\n",
    "   - Fine-tuned SBERT: contrastive training on pseudo-labels.\n",
    "7. **Evaluation**\n",
    "   - Compare similarity distributions of positive vs. negative pairs.\n",
    "   - Visualise results and inspect qualitative example rankings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import kagglehub\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample, losses\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## CV Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"suriyaganesh/resume-dataset-structured\"\n",
    "final_path = kagglehub.dataset_download(dataset_id)\n",
    "# DATA_DIR = Path(final_path)  # FOR MY VS CODE\n",
    "DATA_DIR = Path(\"/kaggle/input/resume-dataset-structured\")\n",
    "\n",
    "people = pd.read_csv(DATA_DIR / \"01_people.csv\")\n",
    "abilities = pd.read_csv(DATA_DIR / \"02_abilities.csv\")\n",
    "education = pd.read_csv(DATA_DIR / \"03_education.csv\")\n",
    "experience = pd.read_csv(DATA_DIR / \"04_experience.csv\")\n",
    "person_skills = pd.read_csv(DATA_DIR / \"05_person_skills.csv\")\n",
    "\n",
    "print(\"people.columns =\", people.columns)\n",
    "print(\"abilities.columns =\", abilities.columns)\n",
    "print(\"education.columns =\", education.columns)\n",
    "print(\"experience.columns =\", experience.columns)\n",
    "print(\"person_skills.columns =\", person_skills.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Create CV Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "We merged the multiple resume dataframes based on the primary key \"person_id\" to create a comprehensive dataframe containing all relevant information about each candidate:\n",
    "- Past experiences (Job titles)\n",
    "- Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "experience_agg = (\n",
    "    experience.groupby(\"person_id\")[\"title\"].apply(list).reset_index(name=\"title\")\n",
    ")\n",
    "\n",
    "skills_agg = (\n",
    "    person_skills.groupby(\"person_id\")[\"skill\"].apply(list).reset_index(name=\"skills\")\n",
    ")\n",
    "\n",
    "df_resume = people.merge(experience_agg, on=\"person_id\", how=\"left\").merge(\n",
    "    skills_agg, on=\"person_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename and drop columns\n",
    "df_resume = df_resume.drop(columns=[\"email\", \"phone\", \"linkedin\", \"name\"])\n",
    "df_resume = df_resume.rename(\n",
    "    columns={\n",
    "        \"title\": \"past_experience\",\n",
    "        \"skill\": \"technical_skills\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(df_resume.columns)\n",
    "print(df_resume.shape)\n",
    "df_resume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Past Experience Column EDA & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Removing empty rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_experience_count = df_resume[\"past_experience\"].isna().sum()\n",
    "print(f\"past_experience null rows: {na_experience_count}\")\n",
    "na_skills_count = df_resume[\"skills\"].isna().sum()\n",
    "print(f\"technical_skills null rows: {na_skills_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NA\n",
    "df_resume = df_resume.dropna(subset=[\"past_experience\", \"skills\"])\n",
    "\n",
    "# Verify no NA values\n",
    "na_experience_count = df_resume[\"past_experience\"].isna().sum()\n",
    "print(f\"past_experience null rows: {na_experience_count}\")\n",
    "na_skills_count = df_resume[\"skills\"].isna().sum()\n",
    "print(f\"technical_skills null rows: {na_skills_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Data Exploration of *past_experience* column before cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = [title for titles in df_resume[\"past_experience\"] for title in titles]\n",
    "unique_job_titles = len(set(all_titles))\n",
    "print(f\"Number of unique job titles before cleaning: {unique_job_titles}\")\n",
    "\n",
    "\n",
    "avg_past_experiences = round(\n",
    "    np.mean([len(titles) for titles in df_resume[\"past_experience\"]]), 2\n",
    ")\n",
    "print(f\"Average number of past jobs per candidate: {avg_past_experiences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Cleaning and normalizing *past_experience* column by removing special characters, replacing abbreviations and converting to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACRONYMS = [\n",
    "    \"SQL\",\n",
    "    \"IT\",\n",
    "    \"UI\",\n",
    "    \"UX\",\n",
    "    \"API\",\n",
    "    \"ETL\",\n",
    "    \"AWS\",\n",
    "    \"IOS\",\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_past_experience(x):\n",
    "    \"\"\"\n",
    "    Ensures past_experience becomes a clean list of raw title strings,\n",
    "    splitting on commas if necessary.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        titles = []\n",
    "        for s in x:\n",
    "            for t in str(s).split(\",\"):\n",
    "                t = t.strip()\n",
    "                if t:\n",
    "                    titles.append(t)\n",
    "        return titles\n",
    "\n",
    "    elif isinstance(x, str):\n",
    "        return [t.strip() for t in x.split(\",\") if t.strip()]\n",
    "\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_title(t):\n",
    "    t = str(t).strip()\n",
    "    t = re.sub(r\"\\.\", \"\", t)\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"\\s*-\\s*\", \" \", t)  # replace hyphens with spaces\n",
    "\n",
    "    replacements = {\n",
    "        r\"\\bdba\\b\": \"database administrator\",\n",
    "        r\"\\bdb\\b\": \"database\",\n",
    "        r\"\\bsr\\b\": \"senior\",\n",
    "        r\"\\bjr\\b\": \"junior\",\n",
    "        r\"\\bdev\\b\": \"developer\",\n",
    "        r\"\\beng\\b\": \"engineer\",\n",
    "        r\"\\badmin\\b\": \"administrator\",\n",
    "        r\"\\barch\\b\": \"architect\",\n",
    "        r\"\\bpm\\b\": \"project manager\",\n",
    "        r\"\\bqa\\b\": \"quality assurance\",\n",
    "        r\"\\bbi\\b\": \"business intelligence\",\n",
    "        r\"\\bfs\\b\": \"full stack\",\n",
    "        r\"\\bceo\\b\": \"chief executive officer\",\n",
    "        r\"\\bhr\\b\": \"human resource\",\n",
    "        r\"\\bai\\b\": \"artificial intelligence\",\n",
    "        r\"\\bml\\b\": \"machine learning\",\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        t = re.sub(pattern, replacement, t)\n",
    "    t = t.title()\n",
    "    for acronym in ACRONYMS:\n",
    "        t = re.sub(rf\"\\b{acronym.title()}\\b\", acronym, t)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "def clean_and_dedupe_past_experience(past_titles: list[str]) -> str:\n",
    "    \"\"\"past_titles is a list of job title strings.\"\"\"\n",
    "    if not isinstance(past_titles, list):\n",
    "        return \"\"\n",
    "    cleaned = [clean_title(t) for t in past_titles if isinstance(t, str) and t.strip()]\n",
    "    unique_titles = list(dict.fromkeys(cleaned))\n",
    "    return \", \".join(unique_titles)\n",
    "\n",
    "\n",
    "df_resume[\"past_experience\"] = (\n",
    "    df_resume[\"past_experience\"]\n",
    "    .apply(normalize_past_experience)\n",
    "    .apply(clean_and_dedupe_past_experience)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Data Exploration of *past_experience* column after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = [\n",
    "    title for titles in df_resume[\"past_experience\"] for title in titles.split(\", \")\n",
    "]\n",
    "unique_job_titles = len(set(all_titles))\n",
    "\n",
    "print(f\"Unique cleaned job titles: {unique_job_titles}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Skills Column EDA & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Data Exploration of *skills* column before cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_resume[\"skills\"].sample(5, random_state=42)\n",
    "\n",
    "for i, skills in samples.items():\n",
    "    print(f\"person_id {i} | Skills: {skills}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "The raw *skills* column mixes true technical skills with long, responsibility-style sentences.  \n",
    "To make this field usable for modelling, we extract only **atomic, tool-level skills**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "##### Cleaning *skills* column:\n",
    "\n",
    "**Why Cleaning Is Needed?**\n",
    "- Skills include **verbose experience bullet points** (“Develop strategic direction for the information systems…”).  \n",
    "- These entries are inconsistent, non-technical, and not suitable as skill tokens.  \n",
    "- Some resumes contain **hundreds of tokens**, creating noise and memory issues\n",
    "\n",
    "**Cleaning Heuristic (Rule-Based)**\n",
    "We remove entries that:\n",
    "- Are **too long** or contain many words.\n",
    "- End with **sentence-like punctuation**.\n",
    "- Contain common **responsibility verbs** (“managed”, “implemented”, “maintained”, etc.).\n",
    "- Begin with **task verbs** (“identify”, “review”, “create”, “plan”, “test”…).\n",
    "\n",
    "Then we:\n",
    "- **Normalize** acronyms and variants (e.g. *js -> JavaScript*, consistent casing).\n",
    "- **Deduplicate** skills within each resume.\n",
    "\n",
    "**Handling Outlier Resumes**\n",
    "Resumes with excessively large skill lists are dropped because they:\n",
    "- Act as **outliers** during training.\n",
    "- Create **memory/compute spikes** for BERT embeddings.\n",
    "- Harm similarity learning by inflating token noise.\n",
    "\n",
    "The resulting *clean_skills* field contains concise, consistent, modelling-ready skill tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACRONYMS = [\n",
    "    \"SQL\",\n",
    "    \"IT\",\n",
    "    \"UI\",\n",
    "    \"UX\",\n",
    "    \"API\",\n",
    "    \"ETL\",\n",
    "    \"AWS\",\n",
    "    \"IOS\",\n",
    "    \"XML\",\n",
    "    \"HTML\",\n",
    "    \"CSS\",\n",
    "    \"PHP\",\n",
    "]\n",
    "\n",
    "SPECIAL_MAP = {\n",
    "    \"js\": \"javascript\",\n",
    "    \"py\": \"python\",\n",
    "    \"ms\": \"microsoft\",\n",
    "    \"node.js\": \"nodejs\",\n",
    "    \"react.js\": \"react\",\n",
    "}\n",
    "\n",
    "\n",
    "EXPERIENCE_VERBS = [\n",
    "    \"managed\",\n",
    "    \"maintained\",\n",
    "    \"responsible\",\n",
    "    \"implemented\",\n",
    "    \"supported\",\n",
    "    \"provided\",\n",
    "    \"ensured\",\n",
    "    \"analyzed\",\n",
    "    \"resolved\",\n",
    "    \"troubleshot\",\n",
    "    \"troubleshooting\",\n",
    "    \"trained\",\n",
    "    \"developed\",\n",
    "    \"installed\",\n",
    "    \"configured\",\n",
    "    \"handled\",\n",
    "    \"engaged\",\n",
    "    \"drove\",\n",
    "    \"communicated\",\n",
    "    \"explained\",\n",
    "    \"assumed\",\n",
    "    \"acted\",\n",
    "    \"diagnosed\",\n",
    "    \"and\",\n",
    "]\n",
    "\n",
    "START_VERBS = [\n",
    "    \"identify\",\n",
    "    \"review\",\n",
    "    \"gather\",\n",
    "    \"create\",\n",
    "    \"document\",\n",
    "    \"perform\",\n",
    "    \"schedule\",\n",
    "    \"plan\",\n",
    "    \"participate\",\n",
    "    \"execute\",\n",
    "    \"lead\",\n",
    "    \"work\",\n",
    "    \"test\",\n",
    "    \"approve\",\n",
    "    \"approved\",\n",
    "    \"confirmed\",\n",
    "    \"tracked\",\n",
    "    \"monitored\",\n",
    "    \"controlled\",\n",
    "    \"managed\",\n",
    "    \"evaluated\",\n",
    "    \"defined\",\n",
    "    \"formulated\",\n",
    "    \"assembled\",\n",
    "    \"coordinated\",\n",
    "    \"follow\",\n",
    "    \"followed\",\n",
    "    \"upload\",\n",
    "    \"uploaded\",\n",
    "    \"research\",\n",
    "    \"suggested\",\n",
    "    \"verified\",\n",
    "]\n",
    "\n",
    "verb_pattern = re.compile(r\"\\b(\" + \"|\".join(EXPERIENCE_VERBS) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def normalize_skill(s):\n",
    "    \"\"\"Light normalization, safe for NaNs, suitable for BERT input.\"\"\"\n",
    "    if s is None or (isinstance(s, float) and math.isnan(s)):\n",
    "        return \"\"\n",
    "\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    s_lower = s.lower()\n",
    "\n",
    "    if s_lower in SPECIAL_MAP:\n",
    "        return SPECIAL_MAP[s_lower]\n",
    "\n",
    "    for ac in ACRONYMS:\n",
    "        if s_lower == ac.lower():\n",
    "            return ac\n",
    "\n",
    "    return s_lower\n",
    "\n",
    "\n",
    "def is_valid_skill(s):\n",
    "    \"\"\"Filter out experience-style sentences, keep short skill-like tokens.\"\"\"\n",
    "    if not s:\n",
    "        return False\n",
    "    s_strip = s.strip()\n",
    "    words = s_strip.split()\n",
    "\n",
    "    if len(words) > 6:\n",
    "        return False\n",
    "    if len(s_strip) > 80:\n",
    "        return False\n",
    "\n",
    "    first_word = words[0].lower()\n",
    "    if first_word in START_VERBS:\n",
    "        return False\n",
    "\n",
    "    if s_strip[-1] in \".!?\":\n",
    "        return False\n",
    "\n",
    "    if verb_pattern.search(s_strip) and len(words) > 5:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_skill_list(skills_list):\n",
    "    \"\"\"Apply normalization + filtering + simple dedupe for one resume.\"\"\"\n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "\n",
    "    for raw in skills_list:\n",
    "        norm = normalize_skill(raw)\n",
    "        if is_valid_skill(norm):\n",
    "            if norm not in seen:\n",
    "                seen.add(norm)\n",
    "                cleaned.append(norm)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Apply to your dataframe\n",
    "df_resume[\"clean_skills\"] = df_resume[\"skills\"].apply(clean_skill_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Data Exploration of *skills* column after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_resume[\"clean_skills\"].sample(5, random_state=42)\n",
    "\n",
    "for person_id, skills in samples.items():\n",
    "    print(f\"person_id {person_id} | Skills: {skills}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = df_resume.drop(columns=[\"skills\"])\n",
    "\n",
    "# drop rows with empty clean_skills\n",
    "df_resume = df_resume[df_resume[\"clean_skills\"].map(len) > 0]\n",
    "empty_skills_count = (df_resume[\"clean_skills\"].map(len) == 0).sum()\n",
    "print(f\"Rows with empty clean_skills: {empty_skills_count}\")\n",
    "\n",
    "avg_skills_per_candidate = round(\n",
    "    np.mean([len(skills) for skills in df_resume[\"clean_skills\"]]), 2\n",
    ")\n",
    "print(f\"Average number of skills per candidate: {avg_skills_per_candidate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Removing Low-Information CVs\n",
    "\n",
    "After cleaning the skills field, we remove resumes with **fewer than four skills**, because they:\n",
    "\n",
    "- Usually come from **noisy scraping** or from removing **responsibility-style sentences** during cleaning.\n",
    "- Contain **too little information** to represent a candidate meaningfully.\n",
    "- Act as **outliers** that destabilise similarity scores and BERT embeddings.\n",
    "- Reduce the overall **quality and consistency** of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = df_resume[df_resume[\"clean_skills\"].apply(lambda x: len(x) >= 4)]\n",
    "print(df_resume.shape)\n",
    "df_resume.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = df_resume[\"clean_skills\"].explode()\n",
    "unique_skills = all_skills.nunique()\n",
    "print(f\"Number of unique skills after cleaning: {unique_skills}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Data Exploration of entire CV dataset after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Remove duplicate CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we must convert the list to a tuple to make it hashable for duplication checking\n",
    "df_resume[\"clean_skills_tuple\"] = df_resume[\"clean_skills\"].apply(\n",
    "    lambda x: tuple(sorted(x))\n",
    ")\n",
    "\n",
    "dup_counts = df_resume.duplicated(\n",
    "    subset=[\"past_experience\", \"clean_skills_tuple\"]\n",
    ").sum()\n",
    "\n",
    "print(f\"Number of duplicate resumes: {dup_counts}\")\n",
    "df_resume = df_resume.drop_duplicates(\n",
    "    subset=[\"past_experience\", \"clean_skills_tuple\"],\n",
    "    keep=\"first\",\n",
    ")\n",
    "# drop the helper column\n",
    "df_resume = df_resume.drop(columns=[\"clean_skills_tuple\"])\n",
    "\n",
    "print(f\"Dataframe shape after dropping duplicates: {df_resume.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Visualize Cleaned & Filtered CV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_with_cumulative(\n",
    "    series,\n",
    "    top_n=30,\n",
    "    ylabel=\"Category\",\n",
    "    title=\"Top Categories with Cumulative Percentage\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Make a horizontal bar chart with a cumulative percentage line\n",
    "    for the top_n values of a Series.\n",
    "    \"\"\"\n",
    "    counts = series.value_counts().head(top_n)\n",
    "    total = len(series)\n",
    "    cumulative_pct = (counts.cumsum() / total) * 100\n",
    "    counts_sorted = counts[::-1]\n",
    "    cumulative_pct_sorted = cumulative_pct.reindex(counts_sorted.index)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.barh(\n",
    "        counts_sorted.index,\n",
    "        counts_sorted.values.astype(float),\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Count\", color=\"blue\")\n",
    "    ax1.tick_params(axis=\"x\", labelcolor=\"blue\")\n",
    "    ax1.set_ylabel(ylabel)\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    y_pos = np.arange(len(counts_sorted))\n",
    "\n",
    "    ax2.plot(\n",
    "        cumulative_pct_sorted.to_numpy(),\n",
    "        y_pos,\n",
    "        color=\"darkorange\",\n",
    "        marker=\"o\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        label=\"Cumulative %\",\n",
    "    )\n",
    "\n",
    "    ax2.set_xlabel(\"Cumulative Percentage\", color=\"darkorange\")\n",
    "    ax2.tick_params(axis=\"x\", labelcolor=\"darkorange\")\n",
    "    ax2.set_xlim(0, 100)\n",
    "\n",
    "    for i, percentage in enumerate(cumulative_pct_sorted.values):\n",
    "        ax2.text(\n",
    "            percentage + 1,\n",
    "            i,\n",
    "            f\"{percentage:.1f}%\",\n",
    "            va=\"center\",\n",
    "            ha=\"left\",\n",
    "            color=\"darkorange\",\n",
    "            size=8,\n",
    "        )\n",
    "\n",
    "    ax1.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Visualize final *skills* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_skills = 30\n",
    "\n",
    "# Explode clean_skills\n",
    "skills_long = df_resume[[\"person_id\", \"clean_skills\"]].explode(\"clean_skills\")\n",
    "skills_long = skills_long.rename(columns={\"clean_skills\": \"skill\"})\n",
    "\n",
    "plot_top_with_cumulative(\n",
    "    series=skills_long[\"skill\"],\n",
    "    top_n=number_of_skills,\n",
    "    ylabel=\"Skill\",\n",
    "    title=f\"Top {number_of_skills} Skills with Cumulative Percentage\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Visualze final *past_experience* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_titles = 30\n",
    "df_resume[\"past_experience_list\"] = df_resume[\"past_experience\"].apply(\n",
    "    lambda s: [t.strip() for t in str(s).split(\",\") if t.strip()]\n",
    ")\n",
    "\n",
    "exploded_titles = df_resume.explode(\"past_experience_list\")\n",
    "\n",
    "plot_top_with_cumulative(\n",
    "    series=exploded_titles[\"past_experience_list\"],\n",
    "    top_n=number_of_titles,\n",
    "    ylabel=\"Job Title\",\n",
    "    title=f\"Top {number_of_titles} Job Titles in Past Experience with Cumulative Percentage\",\n",
    ")\n",
    "\n",
    "# drop helper column\n",
    "df_resume = df_resume.drop(columns=[\"past_experience_list\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "#### Dataset Domain Characteristics\n",
    "\n",
    "From the visualisations above it becomes clear that the resume dataset is heavily concentrated in the **tech domain**, including:\n",
    "\n",
    "Tech jobs:\n",
    "- Software development  \n",
    "- Data and analytics roles  \n",
    "- IT infrastructure, networking, and engineering  \n",
    "- Cloud and DevOps-related positions  \n",
    "\n",
    "As well as tech skills:\n",
    "- A strong prevalence of **programming languages** (Python, Java, SQL)\n",
    "- Frequent **cloud / tooling skills** (AWS, Azure, Docker, Git)\n",
    "- Limited representation of **non-technical** roles\n",
    "\n",
    "We acknowledge that the dataset:\n",
    "\n",
    "- Does **not represent all industries or job categories** \n",
    "- May reflect **source-specific biases** from the scraping process\n",
    "\n",
    "However, it still provides a **consistent, well-defined domain** for the purpose of our mini-project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Job Posting Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id_jobs = \"asaniczka/1-3m-linkedin-jobs-and-skills-2024\"\n",
    "final_path_jobs = kagglehub.dataset_download(dataset_id_jobs)\n",
    "# DATA_DIR_JOBS = Path(final_path_jobs)  # FOR MY VS CODE\n",
    "DATA_DIR_JOBS = Path(\"/kaggle/input/1-3m-linkedin-jobs-and-skills-2024\")\n",
    "\n",
    "job_skills = pd.read_csv(DATA_DIR_JOBS / \"job_skills.csv\")\n",
    "linkedin_jobs = pd.read_csv(DATA_DIR_JOBS / \"linkedin_job_postings.csv\")\n",
    "\n",
    "print(\"job_skills.columns =\", job_skills.columns)\n",
    "print(\"linkedin_jobs.columns =\", linkedin_jobs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Create Job Posting Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = linkedin_jobs.merge(job_skills, on=\"job_link\", how=\"left\")\n",
    "df_jobs = df_jobs.drop(\n",
    "    columns=[\n",
    "        \"job_link\",\n",
    "        \"last_processed_time\",\n",
    "        \"got_summary\",\n",
    "        \"got_ner\",\n",
    "        \"is_being_worked\",\n",
    "        \"company\",\n",
    "        \"job_location\",\n",
    "        \"first_seen\",\n",
    "        \"search_city\",\n",
    "        \"search_country\",\n",
    "        \"search_position\",\n",
    "        \"job_level\",\n",
    "        \"job_type\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(df_jobs.shape)\n",
    "df_jobs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Remove missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.dropna()\n",
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Filtering & Sampling the Job Dataframe\n",
    "\n",
    "Before pseudo-labelling, we filter the **1.3M LinkedIn job postings** using tech-specific keywords.  \n",
    "This ensures the job data aligns with the **technical focus** of our resume corpus.\n",
    "\n",
    "**Why filtering is necessary**\n",
    "\n",
    "Without filtering:\n",
    "- Random sampling would return mostly **irrelevant roles** (e.g, retail, nursing, hospitality).  \n",
    "- We would get **almost no meaningful positive CV–job matches**.  \n",
    "- Top-K retrieval would be dominated by **trivial negatives**.  \n",
    "- Contrastive fine-tuning could **collapse**, because the model sees only non-matching pairs and learns nothing useful.\n",
    "\n",
    "**Why filtering is not a problem**\n",
    "- We are **not restricting** the model’s learning ability.  \n",
    "- We are **increasing the signal-to-noise ratio** by focusing on jobs from the same domain as the resumes.  \n",
    "- This helps the model learn **high-quality, domain-relevant semantic patterns** instead of noise.\n",
    "\n",
    "In short:  \n",
    "Filtering the job dataset ensures that pseudo-labelling produces **valid positives**, **meaningful negatives**, and a **stable contrastive learning signal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "TECH_KEYWORDS = [\n",
    "    \"database\",\n",
    "    \"sql\",\n",
    "    \"dba\",\n",
    "    \"data engineer\",\n",
    "    \"data scientist\",\n",
    "    \"data analyst\",\n",
    "    \"software\",\n",
    "    \"developer\",\n",
    "    \"backend\",\n",
    "    \"full stack\",\n",
    "    \"cloud\",\n",
    "    \"aws\",\n",
    "    \"azure\",\n",
    "    \"gcp\",\n",
    "    \"devops\",\n",
    "    \"linux\",\n",
    "    \"oracle\",\n",
    "    \"postgres\",\n",
    "    \"mysql\",\n",
    "    \"network engineer\",\n",
    "    \"systems engineer\",\n",
    "]\n",
    "\n",
    "pattern = \"|\".join(TECH_KEYWORDS)\n",
    "\n",
    "mask = df_jobs[\"job_title\"].str.contains(pattern, case=False, na=False) | df_jobs[\n",
    "    \"job_skills\"\n",
    "].str.contains(pattern, case=False, na=False)\n",
    "\n",
    "df_jobs_relevant = df_jobs[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Total jobs:\", len(df_jobs))\n",
    "print(\"Relevant tech jobs after filtering:\", len(df_jobs_relevant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_relevant.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "**Sampling the Job Corpus**\n",
    "\n",
    "From the ~220k technical job postings remaining after filtering, we **randomly sample 10,000 jobs** to build the job corpus used for candidate retrieval and pseudo-labelling.\n",
    "\n",
    "Random sampling:\n",
    "- **Preserves** the overall distribution of technical job families,\n",
    "- While **drastically reducing** the computational cost of:\n",
    "  - Embedding all job postings,\n",
    "  - Running similarity search,\n",
    "  - And generating CV–job pseudo-labels.\n",
    "\n",
    "A 10k job subset is **large enough** to:\n",
    "- Provide diverse positive and negative pairs,\n",
    "- And ensure meaningful contrastive learning.\n",
    "\n",
    "In practice, this setup produces **~125k pseudo-labelled pairs**, which is well within the typical range used in academic SBERT fine-tuning work (often **10k–300k** labelled/ pseudo-labelled pairs) and is more than sufficient for stable bi-encoder training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 10_000\n",
    "df_jobs_final = df_jobs_relevant.sample(n=N_JOBS, random_state=42).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "print(\"Using jobs:\", len(df_jobs_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Job Posting Columns EDA & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Data Exploration of *job_title* and *job_skills* columns before cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_job_titles_before = df_jobs_final[\"job_title\"].nunique()\n",
    "print(f\"Number of unique job titles before cleaning: {unique_job_titles_before}\")\n",
    "\n",
    "all_job_skills = (\n",
    "    df_jobs_final[\"job_skills\"].astype(str).str.split(\",\").explode().str.strip()\n",
    ")\n",
    "\n",
    "unique_job_skills_before = all_job_skills.nunique()\n",
    "print(f\"Unique job skills before cleaning: {unique_job_skills_before}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "We convert each comma-separated skills string into a list of individual skill phrases and then pass them through our existing skill-cleaning pipeline for the CV dataset cleaning.\n",
    "\n",
    "This pipeline:\n",
    "- Removes responsibility-style sentences and non-skill fragments,\n",
    "- Normalizes acronyms (e.g., “api” → “API”),\n",
    "- Standardizes common variants (“js” → “javascript”),\n",
    "- Deduplicates repeated entries,\n",
    "- Applies length and structure heuristics to keep only true skill tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_final[\"job_title_clean\"] = df_jobs_final[\"job_title\"].apply(clean_title)\n",
    "unique_job_titles_after = df_jobs_final[\"job_title_clean\"].nunique()\n",
    "print(f\"Number of unique job titles after cleaning: {unique_job_titles_after}\")\n",
    "\n",
    "\n",
    "def split_job_skill_string(s):\n",
    "    \"\"\"Convert a comma-separated skill string into a list of skill phrases.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "\n",
    "# Apply existing cleaning pipeline\n",
    "df_jobs_final[\"skill_list_raw\"] = df_jobs_final[\"job_skills\"].apply(\n",
    "    split_job_skill_string\n",
    ")\n",
    "df_jobs_final[\"job_skills_clean\"] = df_jobs_final[\"skill_list_raw\"].apply(\n",
    "    clean_skill_list\n",
    ")\n",
    "unique_skills_after = df_jobs_final[\"job_skills_clean\"].explode().nunique()\n",
    "print(f\"Unique job skills after cleaning: {unique_skills_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "After cleaning, the number of unique job skills dropped by roughly 15,000, effectively removing noisy, inconsistent, or non-technical entries from the original scraped dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Unused columns are dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_final = df_jobs_final.drop(\n",
    "    columns=[\"job_title\", \"job_skills\", \"skill_list_raw\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Visualize Cleaned Job Posting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "Visualze final *job_title_clean* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_final[\"job_title_clean\"]\n",
    "\n",
    "plot_top_with_cumulative(\n",
    "    series=df_jobs_final[\"job_title_clean\"],\n",
    "    top_n=30,\n",
    "    ylabel=\"Job Title\",\n",
    "    title=\"Top 30 Job Titles with Cumulative Percentage\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "Visualze final *job_skills_clean* columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_skills_long = df_jobs_final[\"job_skills_clean\"].explode()\n",
    "\n",
    "plot_top_with_cumulative(\n",
    "    series=job_skills_long,\n",
    "    top_n=30,\n",
    "    ylabel=\"Skill\",\n",
    "    title=\"Top 30 Job Skills with Cumulative Percentage\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## Creating the Pseudo-Labelled CV–Job Matching Dataset\n",
    "\n",
    "### Overview of the Pipeline\n",
    "We construct pseudo-labelled CV–job pairs using lightweight heuristics and structured text inputs:\n",
    "\n",
    "- Build **label-documents** for CVs and jobs (titles + skills)\n",
    "- Create **skill and title token sets** for overlap computation\n",
    "- Embed CVs and jobs with a base SBERT model and retrieve top-K jobs per CV\n",
    "- Compute **skill and title overlaps** for each top-K CV–job pair\n",
    "- Label top-K pairs with strong overlap as **positives** and non-top-K pairs with no overlap as **negatives**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### Creating Label Documents and Skill/Title Sets for Pseudo-Labelling\n",
    "\n",
    "To prepare resumes and job postings for overlap-based matching and BERT encoding, we apply the following steps:\n",
    "\n",
    "- Define skill list to set conversion function\n",
    "\n",
    "- Define skill overlap computation function\n",
    "\n",
    "- Define normalization and tokenization function for job titles\n",
    "\n",
    "- Define title overlap computation function\n",
    "\n",
    "- Build structured label documents for BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_set(x):\n",
    "    if isinstance(x, list):\n",
    "        return set(s.lower() for s in x)\n",
    "    return set()\n",
    "\n",
    "\n",
    "def compute_skill_overlap(cv_set: set[str], job_set: set[str]) -> int:\n",
    "    return len(cv_set & job_set)\n",
    "\n",
    "\n",
    "TITLE_STOPWORDS = {\n",
    "    \"senior\",\n",
    "    \"junior\",\n",
    "    \"lead\",\n",
    "    \"assistant\",\n",
    "    \"intern\",\n",
    "    \"and\",\n",
    "    \"&\",\n",
    "    \"-\",\n",
    "    \"/\",\n",
    "}\n",
    "\n",
    "\n",
    "def title_to_tokens(title: str) -> set[str]:\n",
    "    t = str(title).lower()\n",
    "    t = re.sub(r\"[|/,]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    tokens = [tok for tok in t.split(\" \") if tok and tok not in TITLE_STOPWORDS]\n",
    "    return set(tokens)\n",
    "\n",
    "\n",
    "def compute_title_overlap(cv_title: str, job_title: str) -> int:\n",
    "    return len(title_to_tokens(cv_title) & title_to_tokens(job_title))\n",
    "\n",
    "\n",
    "def build_cv_text(row) -> str:\n",
    "    past = str(row[\"past_experience\"]).strip()\n",
    "    skills = \", \".join(row[\"clean_skills\"])\n",
    "    return f\"Past experience: {past}. Technical skills: {skills}.\"\n",
    "\n",
    "\n",
    "def build_job_text(row) -> str:\n",
    "    title = str(row[\"job_title_clean\"]).strip()\n",
    "    skills = \", \".join(row[\"job_skills_clean\"])\n",
    "    return f\"Job title: {title}. Required skills: {skills}.\"\n",
    "\n",
    "\n",
    "cv_df = df_resume.copy()\n",
    "jobs_df = df_jobs_final.copy()\n",
    "\n",
    "\n",
    "# Convert skill lists → sets for overlap computation\n",
    "cv_df[\"skill_set\"] = cv_df[\"clean_skills\"].apply(list_to_set)\n",
    "jobs_df[\"skill_set\"] = jobs_df[\"job_skills_clean\"].apply(list_to_set)\n",
    "\n",
    "# Build text for BERT encoder\n",
    "cv_df[\"label_doc\"] = cv_df.apply(build_cv_text, axis=1)\n",
    "jobs_df[\"label_doc\"] = jobs_df.apply(build_job_text, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Save processed dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.to_csv(\"processed_cv_data.csv\", index=False)\n",
    "jobs_df.to_csv(\"processed_job_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### Embedding label documents & computing cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "We load a pre-trained **Sentence-BERT** model (`msmarco-bert-base-dot-v5`), which embeds text into a vector space where semantically similar documents lie close together.\n",
    "\n",
    "We apply this encoder separately to:\n",
    "- all CV **label documents**\n",
    "- all job **label documents**\n",
    "\n",
    "This produces two embedding matrices: one for CVs and one for jobs.\n",
    "\n",
    "Using these embeddings, we compute a **cosine similarity matrix**:\n",
    "- Each row = a CV  \n",
    "- Each column = a job posting  \n",
    "- Each value = how semantically similar the CV and job are\n",
    "\n",
    "High similarity values indicate semantic similarity between CV and job (potential **positive matches**), while low values indicate semantic differences between CV and job (potential **negative matches**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SentenceTransformer(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "ft_model = SentenceTransformer(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "cv_texts = cv_df[\"label_doc\"].tolist()\n",
    "job_texts = jobs_df[\"label_doc\"].tolist()\n",
    "\n",
    "cv_emb = base_model.encode(\n",
    "    cv_texts, batch_size=64, convert_to_tensor=True, show_progress_bar=True\n",
    ")\n",
    "job_emb = base_model.encode(\n",
    "    job_texts, batch_size=64, convert_to_tensor=True, show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Cosine similarity matrix: shape (num_cvs, num_jobs)\n",
    "S: Tensor = util.cos_sim(cv_emb, job_emb)\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### Creating Pseudo-Labels\n",
    "\n",
    "To generate training data for contrastive fine-tuning, we create positive and negative CV–job pairs using SBERT similarity and simple overlap rules.\n",
    "\n",
    "**Prepare skill sets**  \n",
    "Convert string sets into sets to enable fast overlap checks.\n",
    "\n",
    "**Define thresholds for labelling**\n",
    "- `TOP_K = 50` most similar jobs for each CV considered for positive labels\n",
    "- Positives require: **≥3 skill-token overlaps** and **≥1 title-token overlap**  \n",
    "- Cap per CV: **max 5 positives** and **max 5 negatives**  \n",
    "- Negatives sampled from a **random pool of 200** non–Top-K jobs\n",
    "\n",
    "**Positive pseudo-labels**  \n",
    "For each CV, take the Top-K most similar jobs (via SBERT cosine similarity) and keep only those that satisfy the skill/title overlap thresholds.  \n",
    "\n",
    "**Negative pseudo-labels**  \n",
    "Sample jobs outside the Top-K pool and keep only those with **zero** skill and title overlap. \n",
    "\n",
    "We sample outside Top-K to ensure negatives  are truly non-matching by selecting jobs with zero skill and title overlap, this makes the contrastive training signal much cleaner.\n",
    "\n",
    " **Output**  \n",
    "All collected pairs are stored in `pseudo_pairs`, forming a balanced set of high-quality positives and negatives for fine-tuning, containg a subset of the full CV and job datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df[\"skill_set\"] = cv_df[\"skill_set\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "jobs_df[\"skill_set\"] = jobs_df[\"skill_set\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "TOP_K = 50\n",
    "\n",
    "POS_MIN_SKILLS = 3\n",
    "POS_MIN_TITLE_TOKENS = 1\n",
    "\n",
    "MAX_POS_PER_CV = 5\n",
    "MAX_NEG_PER_CV = 5\n",
    "RANDOM_NEG_POOL = 200\n",
    "\n",
    "pseudo_pairs = []\n",
    "\n",
    "num_cvs = cv_df.shape[0]\n",
    "num_jobs = jobs_df.shape[0]\n",
    "\n",
    "for i in tqdm(range(num_cvs), desc=\"Pseudo-labelling CVs\"):\n",
    "    cv_row = cv_df.iloc[i]\n",
    "    cv_text = cv_row[\"label_doc\"]\n",
    "    cv_skills = cv_row[\"skill_set\"]\n",
    "    cv_title = str(cv_row[\"past_experience\"]).split(\",\")[0]\n",
    "\n",
    "    sims = S[i].cpu().numpy()\n",
    "\n",
    "    # POSITIVES\n",
    "    top_k_idx = np.argsort(-sims)[:TOP_K]\n",
    "    pos_count = 0\n",
    "\n",
    "    for j in top_k_idx:\n",
    "        job_row = jobs_df.iloc[j]\n",
    "        job_text = job_row[\"label_doc\"]\n",
    "        job_skills = job_row[\"skill_set\"]\n",
    "        job_title = job_row[\"job_title_clean\"]\n",
    "\n",
    "        skill_overlap = compute_skill_overlap(cv_skills, job_skills)\n",
    "        title_overlap = compute_title_overlap(cv_title, job_title)\n",
    "        if (skill_overlap >= POS_MIN_SKILLS) and (\n",
    "            title_overlap >= POS_MIN_TITLE_TOKENS\n",
    "        ):\n",
    "            pseudo_pairs.append((cv_text, job_text, 1))\n",
    "            pos_count += 1\n",
    "            if pos_count >= MAX_POS_PER_CV:\n",
    "                break\n",
    "\n",
    "    # NEGATIVES\n",
    "    neg_count = 0\n",
    "\n",
    "    all_idx = np.arange(num_jobs)\n",
    "    neg_pool = np.setdiff1d(all_idx, top_k_idx)\n",
    "\n",
    "    if len(neg_pool) == 0:\n",
    "        continue\n",
    "    rand_idx = np.random.choice(\n",
    "        neg_pool,\n",
    "        size=min(RANDOM_NEG_POOL, len(neg_pool)),\n",
    "        replace=False,\n",
    "    )\n",
    "\n",
    "    for j in rand_idx:\n",
    "        job_row = jobs_df.iloc[j]\n",
    "        job_text = job_row[\"label_doc\"]\n",
    "        job_skills = job_row[\"skill_set\"]\n",
    "        job_title = job_row[\"job_title_clean\"]\n",
    "\n",
    "        skill_overlap = compute_skill_overlap(cv_skills, job_skills)\n",
    "        title_overlap = compute_title_overlap(cv_title, job_title)\n",
    "\n",
    "        if skill_overlap == 0 and title_overlap == 0:\n",
    "            pseudo_pairs.append((cv_text, job_text, 0))\n",
    "            neg_count += 1\n",
    "            if neg_count >= MAX_NEG_PER_CV:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = pd.DataFrame(pseudo_pairs, columns=[\"cv_text\", \"job_text\", \"label\"])\n",
    "print(pairs_df[\"label\"].value_counts())\n",
    "pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairs_df[pairs_df[\"label\"] == 1].sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairs_df[pairs_df[\"label\"] == 0].sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "## Fine-Tuning the BERT Bi-Encoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "##### **Model:** Pre-Trained Sentence-BERT\n",
    "\n",
    "We start from a pre-trained **Sentence-BERT bi-encoder** model: `msmarco-bert-base-dot-v5`\n",
    "\n",
    "Each training example consists of: '(cv_label_doc, job_label_doc, binary_label)'\n",
    "\n",
    "We convert these examples into `InputExample` objects and feed them into a mini-batch training loop.\n",
    "\n",
    "We use **CosineSimilarityLoss** in a contrastive learning setup, which:\n",
    "\n",
    "- Encourages **high cosine similarity** for **positive pairs** (label = 1)\n",
    "- Encourages **low cosine similarity** for **negative pairs** (label = 0)\n",
    "\n",
    "##### **Mechanism:** Contrastive Learning\n",
    "\n",
    "1. The bi-encoder maps both texts (CV and job posting) into embeddings.\n",
    "2. We compute the cosine similarity between the two embeddings.\n",
    "3. The loss then pushes this similarity score towards the target label:\n",
    "   - `1.0` for matches  \n",
    "   - `0.0` for non-matches  \n",
    "\n",
    "Over many such examples, the bi-encoder learns an **HR-specific similarity space**, where CVs that fit a job are closer to it than unrelated resumes.\n",
    "\n",
    "#### **Class Balancing:**\n",
    "\n",
    "The dataset was heavily imbalanced (≈85k negatives vs. 40k positives), so we applied balanced sampling by capping each class at 25,000 examples.\n",
    "\n",
    "This ensured that the fine-tuning process was not dominated by negative pairs and that both classes contributed equally to the contrastive objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### Environment Configuration on Kaggle\n",
    "\n",
    "On Kaggle, we configure two environment variables to avoid common execution issues:\n",
    "\n",
    "- **Select the correct GPU:**  \n",
    "  `CUDA_VISIBLE_DEVICES=\"0\"` ensures that PyTorch uses Kaggle’s single available GPU. This prevents the notebook from accidentally trying to use a non-existent device.\n",
    "\n",
    "- **Turn off Weights & Biases tracking:**  \n",
    "  `WANDB_DISABLED=\"true\"` stops the WandB tracking service from starting. Kaggle often blocks external network traffic, and this was done to prevent WandB from freezing or interrupting training (which it was doing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### Train–Test Split\n",
    "\n",
    "We apply a strict splitting strategy where **both CVs and job postings are completely disjoint** between the training and validation sets.\n",
    "\n",
    "- No CV appearing in training appears in validation  \n",
    "- No job posting appearing in training appears in validation  \n",
    "- No pair or text fragment is shared across splits  \n",
    "\n",
    "This setup removes all risk of **data leakage**, ensuring the model cannot rely on memorized CV wording or recurring job templates. \n",
    "\n",
    "Instead, it must **generalize to entirely unseen CVs and unseen job descriptions**, providing a robust and unbiased evaluation of real-world matching performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance dataset through downsampling\n",
    "target_per_class_strict = 25000\n",
    "pairs_balanced = pairs_df.groupby(\"label\", group_keys=False).apply(\n",
    "    lambda g: g.sample(n=min(len(g), target_per_class_strict), random_state=42)\n",
    ")\n",
    "\n",
    "# unique CVs and Jobs from the balanced pairs\n",
    "unique_cvs = pairs_balanced[\"cv_text\"].unique()\n",
    "unique_jobs = pairs_balanced[\"job_text\"].unique()\n",
    "\n",
    "# split independently into train/val sets\n",
    "train_cvs, val_cvs = train_test_split(\n",
    "    unique_cvs,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_jobs, val_jobs = train_test_split(\n",
    "    unique_jobs,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# enforce strict separation\n",
    "train_df = pairs_balanced[\n",
    "    pairs_balanced[\"cv_text\"].isin(train_cvs)\n",
    "    & pairs_balanced[\"job_text\"].isin(train_jobs)\n",
    "].copy()\n",
    "\n",
    "val_df = pairs_balanced[\n",
    "    pairs_balanced[\"cv_text\"].isin(val_cvs) & pairs_balanced[\"job_text\"].isin(val_jobs)\n",
    "].copy()\n",
    "\n",
    "print(\"Strict train pairs:\", len(train_df))\n",
    "print(\"Strict val pairs:\", len(val_df))\n",
    "\n",
    "print(\"Train label balance:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Val label balance:\\n\", val_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "Verifying the splits are disjoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_set = set(train_df[\"cv_text\"])\n",
    "val_cv_set = set(val_df[\"cv_text\"])\n",
    "print(\"CV overlap:\", len(train_cv_set & val_cv_set))\n",
    "\n",
    "train_job_set = set(train_df[\"job_text\"])\n",
    "val_job_set = set(val_df[\"job_text\"])\n",
    "print(\"Job overlap:\", len(train_job_set & val_job_set))\n",
    "\n",
    "train_pairs = set(zip(train_df[\"cv_text\"], train_df[\"job_text\"]))\n",
    "val_pairs = set(zip(val_df[\"cv_text\"], val_df[\"job_text\"]))\n",
    "print(\"Pair overlap:\", len(train_pairs & val_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "Check CUDA availability and the current device of `ft_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "try:\n",
    "    print(\"Model device:\", next(ft_model.parameters()).device)\n",
    "except Exception as e:\n",
    "    print(\"Could not inspect model device:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "Analyzing token-length distribution of CVs and job postings using same tokenizer as S-BERT model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same tokenizer as SBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/msmarco-bert-base-dot-v5\"\n",
    ")\n",
    "\n",
    "\n",
    "def compute_lengths(texts):\n",
    "    return [len(tokenizer.encode(str(t), add_special_tokens=True)) for t in texts]\n",
    "\n",
    "\n",
    "cv_lengths = compute_lengths(cv_df[\"label_doc\"])\n",
    "job_lengths = compute_lengths(jobs_df[\"label_doc\"])\n",
    "\n",
    "cv_lengths = np.array(cv_lengths)\n",
    "job_lengths = np.array(job_lengths)\n",
    "\n",
    "print(\"CV LENGTHS\")\n",
    "print(\"Mean:\", cv_lengths.mean())\n",
    "print(\"Median:\", np.median(cv_lengths))\n",
    "print(\"95th percentile:\", np.percentile(cv_lengths, 95))\n",
    "print(\"Max:\", cv_lengths.max())\n",
    "\n",
    "print(\"\\nJOB LENGTHS\")\n",
    "print(\"Mean:\", job_lengths.mean())\n",
    "print(\"Median:\", np.median(job_lengths))\n",
    "print(\"95th percentile:\", np.percentile(job_lengths, 95))\n",
    "print(\"Max:\", job_lengths.max())\n",
    "\n",
    "thresholds = [32, 50, 64, 128, 256]\n",
    "\n",
    "print(\"CV LENGTH DISTRIBUTION\")\n",
    "for t in thresholds:\n",
    "    print(f\"{t} tokens:\", (cv_lengths >= t).mean() * 100, \"%\")\n",
    "\n",
    "print(\"\\nJOB LENGTH DISTRIBUTION\")\n",
    "for t in thresholds:\n",
    "    print(f\"{t} tokens:\", (job_lengths >= t).mean() * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "To fine-tune the Sentence-BERT model on the strictly disjoint CV–job dataset, we prepared the training pipeline with three key steps:\n",
    "\n",
    "**1. Selecting Maximum Sequence Length**\n",
    "\n",
    "Token-length analysis showed that CVs reached **1200 tokens** and job postings up to **800 tokens**.  \n",
    "Using the default token truncation caused slow CPU-side preprocessing on Kaggle because the tokenizer still processed the **entire long sequence before truncating**.\n",
    "\n",
    "To avoid this bottleneck while keeping enough semantic content, we set **`max_seq_length = 256`**, which is a practical balance between **speed** and **context coverage**.\n",
    "\n",
    "\n",
    "**2. Creating InputExamples & DataLoader**\n",
    "\n",
    "Each training pair (CV text, job text, label) is converted into a Sentence-Transformers `InputExample`, which stores:\n",
    "\n",
    "- `texts=[cv_text, job_text]`\n",
    "- `label ∈ {0.0, 1.0}`\n",
    "\n",
    "These examples are then wrapped in a PyTorch `DataLoader` to ensures clean batching and consistent pairwise training.\n",
    "\n",
    "**3. Running the fine-tuning loop**\n",
    "\n",
    "We train the model,with the following configuration:\n",
    "- **epochs:** 1\n",
    "- **loss:** CosineSimilarityLoss\n",
    "- **warmup:** 10 % of the training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    InputExample(\n",
    "        texts=[row.cv_text, row.job_text],\n",
    "        label=float(row.label),\n",
    "    )\n",
    "    for row in train_df.itertuples(index=False)\n",
    "]\n",
    "print(\"num strict train:\", len(train_examples))\n",
    "\n",
    "# DataLoader\n",
    "train_batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples,\n",
    "    shuffle=True,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0,\n",
    "    collate_fn=ft_model.smart_batching_collate,\n",
    ")\n",
    "\n",
    "# Loss\n",
    "train_loss = losses.CosineSimilarityLoss(ft_model)\n",
    "\n",
    "# Fine-tune\n",
    "epochs = 1\n",
    "warmup_steps = int(0.1 * len(train_dataloader))\n",
    "\n",
    "ft_model.max_seq_length = 256\n",
    "print(\"Current max_seq_length:\", ft_model.max_seq_length)\n",
    "\n",
    "ft_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation\n",
    "\n",
    "To measure the effect of contrastive fine-tuning, we compare two models on a test set of pseudo-labelled CV–job pairs:\n",
    "\n",
    "1. **Baseline SBERT:** `msmarco-bert-base-dot-v5`  \n",
    "2. **Fine-tuned SBERT:** the same model after contrastive fine-tuning on our CV–job dataset.\n",
    "\n",
    "We evaluate performance using **two quantitative metrics**:\n",
    "\n",
    "**1. ROC–AUC (threshold-free ranking metric)**  \n",
    "- Measures how well the model **ranks** true matches above non-matches.  \n",
    "- Computed by checking whether positive pairs *consistently* receive higher similarity scores than negative ones.  \n",
    "- Does **not** require picking a classification threshold.  \n",
    "- Useful when we care about overall ranking quality.\n",
    "\n",
    "**2. Accuracy: cosine similarity threshold = 0.5**\n",
    "- We turn the similarity score into a binary decision:  \n",
    "  - **score ≥ 0.5 → predict “match”**  \n",
    "  - **score < 0.5 → predict “non-match”**  \n",
    "- The prediction is then compared against the **pseudo-labels** for correctness.  \n",
    "- Accuracy reflects how well the model separates positive and negative pairs at this fixed cutoff.\n",
    "\n",
    "Together, these metrics give a clearer picture:  \n",
    "**ROC–AUC evaluates ranking behaviour**, while **accuracy evaluates classification behaviour**.\n",
    " \n",
    "Cosine similarity distributions alone cannot provide this full performance assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_evaluation(val_df, cv_df, jobs_df, S_matrix, ft_model):\n",
    "    \"\"\"\n",
    "    Full evaluation pipeline:\n",
    "    - Baseline SBERT (pretrained)\n",
    "    - Fine-tuned SBERT\n",
    "    - ROC-based optimal threshold\n",
    "    - Accuracy at default and optimal thresholds\n",
    "    \"\"\"\n",
    "    # baseline evaluation (precomputed similarity matrix)\n",
    "    cv_index_map = {text: i for i, text in enumerate(cv_df[\"label_doc\"])}\n",
    "    job_index_map = {text: i for i, text in enumerate(jobs_df[\"label_doc\"])}\n",
    "\n",
    "    baseline_scores = []\n",
    "    labels = val_df[\"label\"].astype(float).to_numpy()\n",
    "\n",
    "    for row in val_df.itertuples(index=False):\n",
    "        cv_idx = cv_index_map[row.cv_text]\n",
    "        job_idx = job_index_map[row.job_text]\n",
    "        baseline_scores.append(float(S_matrix[cv_idx, job_idx]))\n",
    "\n",
    "    baseline_scores = np.array(baseline_scores)\n",
    "    baseline_auc = roc_auc_score(labels, baseline_scores)\n",
    "\n",
    "    # fine-tuned evaluation (encode with trained model)\n",
    "    cv_texts = val_df[\"cv_text\"].tolist()\n",
    "    job_texts = val_df[\"job_text\"].tolist()\n",
    "\n",
    "    ft_cv_emb = ft_model.encode(cv_texts, batch_size=64, convert_to_tensor=True)\n",
    "    ft_job_emb = ft_model.encode(job_texts, batch_size=64, convert_to_tensor=True)\n",
    "    sim_matrix = util.cos_sim(ft_cv_emb, ft_job_emb)\n",
    "    ft_scores = sim_matrix.diag().cpu().numpy()\n",
    "\n",
    "    ft_auc = roc_auc_score(labels, ft_scores)\n",
    "\n",
    "    # 3. ROC curve to get optimal threshold for FT model\n",
    "    fpr, tpr, thresholds = roc_curve(labels, ft_scores)\n",
    "    optimal_idx = (tpr - fpr).argmax()\n",
    "    best_threshold = thresholds[optimal_idx]\n",
    "    ft_acc_opt = accuracy_score(labels, (ft_scores >= best_threshold).astype(float))\n",
    "    baseline_acc_opt = accuracy_score(\n",
    "        labels, (baseline_scores >= best_threshold).astype(float)\n",
    "    )\n",
    "\n",
    "    # summary\n",
    "    print(\"\\nEVALUATION SUMMARY\\n\")\n",
    "\n",
    "    print(\"BASELINE SBERT\")\n",
    "    print(f\"ROC-AUC            : {baseline_auc:.4f}\")\n",
    "    print(f\"Accuracy @optimal  : {baseline_acc_opt:.4f}\")\n",
    "    print()\n",
    "\n",
    "    print(\"FINE-TUNED SBERT\")\n",
    "    print(f\"ROC-AUC            : {ft_auc:.4f}\")\n",
    "    print(f\"Accuracy @optimal  : {ft_acc_opt:.4f}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Optimal threshold  : {best_threshold:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"baseline_scores\": baseline_scores,\n",
    "        \"ft_scores\": ft_scores,\n",
    "        \"labels\": labels,\n",
    "        \"best_threshold\": best_threshold,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_full_evaluation(\n",
    "    val_df,\n",
    "    cv_df,\n",
    "    jobs_df,\n",
    "    S,\n",
    "    ft_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = results[\"baseline_scores\"]\n",
    "ft_scores = results[\"ft_scores\"]\n",
    "val_labels = results[\"labels\"]\n",
    "\n",
    "# a sanity check to ensure both have same length\n",
    "assert len(baseline_scores) == len(ft_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### Quantitative Results\n",
    "\n",
    "Across both metrics, the **fine-tuned bi-encoder outperforms the baseline**:\n",
    "\n",
    "- **Higher ROC–AUC:** \n",
    "    -  The fine-tuned model ranks matching pairs above mismatches slightly better than the baseline SBERT.\n",
    "\n",
    "- **Much Higher Accuracy:** <50% → 98.9%+\n",
    "    - At a fixed optiomal threshold of 0.47, the fine-tuned model produces substantially more correct match/non-match predictions.\n",
    "\n",
    "This confirms that fine-tuning creates a **more discriminative similarity space**, better tailored to HR semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "### Evaluation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(\n",
    "    baseline_scores,\n",
    "    bins=40,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=\"Baseline SBERT\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.hist(\n",
    "    ft_scores, bins=40, alpha=0.6, density=True, label=\"Fine-Tuned SBERT\", color=\"green\"\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Cosine Similarity Scores\\nBaseline vs Fine-Tuned SBERT\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 1D float arrays\n",
    "baseline_scores = np.asarray(baseline_scores, dtype=float).ravel()\n",
    "ft_scores = np.asarray(ft_scores, dtype=float).ravel()\n",
    "val_labels = np.asarray(val_labels, dtype=int).ravel()\n",
    "\n",
    "pos_idx = val_labels == 1\n",
    "neg_idx = val_labels == 0\n",
    "\n",
    "# baseline plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(\n",
    "    baseline_scores[pos_idx],\n",
    "    bins=40,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=\"Baseline Positives\",\n",
    ")\n",
    "plt.hist(\n",
    "    baseline_scores[neg_idx],\n",
    "    bins=40,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=\"Baseline Negatives\",\n",
    ")\n",
    "\n",
    "plt.title(\"Baseline SBERT: Positive vs Negative Score Distribution\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# fine-tuned plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(\n",
    "    ft_scores[pos_idx],\n",
    "    bins=40,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=\"Fine-Tuned Positives\",\n",
    ")\n",
    "plt.hist(\n",
    "    ft_scores[neg_idx],\n",
    "    bins=40,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=\"Fine-Tuned Negatives\",\n",
    ")\n",
    "\n",
    "plt.title(\"Fine-Tuned SBERT: Positive vs Negative Score Distribution\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(name, scores, labels):\n",
    "    scores = np.array(scores)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    pos = scores[labels == 1]\n",
    "    neg = scores[labels == 0]\n",
    "\n",
    "    print(f\"{name} SBERT Score Summary:\")\n",
    "    print(f\"Positives: mean={pos.mean():.4f}, std={pos.std():.4f}, n={len(pos)}\")\n",
    "    print(f\"Negatives: mean={neg.mean():.4f}, std={neg.std():.4f}, n={len(neg)}\")\n",
    "    print(f\"Margin (pos_mean - neg_mean): {(pos.mean() - neg.mean()):.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "summarize(\"BASELINE\", baseline_scores, val_labels)\n",
    "summarize(\"FINE-TUNED\", ft_scores, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "### Discussion of Quantitative Results\n",
    "\n",
    "- **Baseline SBERT ranked pairs well but with little separation in embedding space.**\n",
    "  ROC–AUC was already extremely high (≈ 0.99) because the baseline model usually assigned slightly higher similarity scores to positive pairs than to negative ones. However, the gap between the two groups was very small (mean positives ~0.95 vs. mean negatives ~0.89).\n",
    "\n",
    "- **Overlap in cosine similarity distributions caused low accuracy for base SBERT.**\n",
    " If cosine similarity distributions for positives and negatives overlap heavily, no similarity threshold (like the optimal threshold of 0.47) can separate them well. The baseline SBERT had overlapping distributions (see below), leading to lower accuracy despite good ranking.\n",
    "\n",
    "- **Fine-tuning expanded the distance between positives and negatives.**  \n",
    "  After contrastive training, positive examples clustered near **1.0**, while negative examples were pushed close to **0.0**.  \n",
    "  The distribution gap increased drastically (from **0.064 to 0.91**), making the two classes clearly separable.\n",
    "\n",
    "- **Accuracy improved dramatically after fine-tuning.**  \n",
    "  Because the similarity gap widened, the same threshold (0.47) became meaningful.  \n",
    "  Accuracy increased from **50% → 98.7%**, showing that the fine-tuned model learned a much more discriminative similarity space.\n",
    "\n",
    "- **ROC–AUC remained nearly unchanged.**  \n",
    "  Since the baseline was already ranking positives above negatives correctly, ROC–AUC could not improve much further. Its stability reflects that *ranking quality was already good* and stayed good after fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "## Qualiative Evaluation\n",
    "\n",
    "Testing the model on synthetic data of 1 CV and 5 job postings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_text = \"\"\"\n",
    "I am a Data Analyst with 3+ years of experience working with SQL, Python, and BI tools.\n",
    "I build dashboards in Power BI and Tableau, create ETL pipelines, and work closely with\n",
    "business stakeholders to define KPIs, prepare reports, and automate data workflows.\n",
    "Comfortable with statistics, A/B testing, and presenting findings to non-technical audiences.\n",
    "\"\"\"\n",
    "\n",
    "jobs = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"Data Analyst (Marketing Analytics)\",\n",
    "        \"text\": \"\"\"\n",
    "        We are looking for a Data Analyst to support our marketing team.\n",
    "        The role involves writing SQL to query large datasets, building dashboards in Power BI\n",
    "        or Tableau, and using Python for data cleaning and basic statistics.\n",
    "        You will collaborate with stakeholders to define KPIs and create regular reports.\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"Business Intelligence Developer\",\n",
    "        \"text\": \"\"\"\n",
    "        As a BI Developer you will design and maintain dashboards and reports,\n",
    "        mainly using Power BI and SQL Server. You will work with business users to\n",
    "        understand reporting requirements, build ETL-style data transformations,\n",
    "        and ensure data quality across multiple sources.\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"Machine Learning Engineer\",\n",
    "        \"text\": \"\"\"\n",
    "        We are hiring a Machine Learning Engineer responsible for developing and deploying\n",
    "        ML models in production. The role requires strong Python skills, experience with\n",
    "        frameworks such as TensorFlow or PyTorch, and knowledge of cloud platforms.\n",
    "        SQL is a plus, but the focus is model development and MLOps, not reporting.\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Senior Frontend Engineer (React)\",\n",
    "        \"text\": \"\"\"\n",
    "        We are looking for a Senior Frontend Engineer with deep experience in React,\n",
    "        TypeScript, CSS, and building complex, responsive web applications.\n",
    "        You will work closely with designers to implement UI components and improve\n",
    "        the user experience. No data analysis or BI background is required.\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"Kindergarten Teacher\",\n",
    "        \"text\": \"\"\"\n",
    "        We are seeking a Kindergarten Teacher responsible for planning lessons,\n",
    "        supporting early childhood development, communicating with parents,\n",
    "        and creating a safe and engaging learning environment.\n",
    "        No technical or data-related skills are required.\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pair(model, cv_text: str, job_text: str) -> float:\n",
    "    cv_emb = model.encode(cv_text, convert_to_tensor=True)\n",
    "    job_emb = model.encode(job_text, convert_to_tensor=True)\n",
    "    sim = util.cos_sim(cv_emb, job_emb).item()\n",
    "    return float(sim)\n",
    "\n",
    "\n",
    "def compare_baseline_vs_ft(cv_text, jobs, base_model, ft_model):\n",
    "    rows = []\n",
    "    for job in jobs:\n",
    "        b = score_pair(base_model, cv_text, job[\"text\"])\n",
    "        f = score_pair(ft_model, cv_text, job[\"text\"])\n",
    "        rows.append((job[\"id\"], job[\"title\"], b, f))\n",
    "    rows_sorted = sorted(rows, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "    print(\"ID | Title                              | Baseline  | Fine-tuned\")\n",
    "    print(\"-\" * 70)\n",
    "    for jid, title, b, f in rows_sorted:\n",
    "        print(f\"{jid:<2} | {title:<32} | {b:8.3f} | {f:10.3f}\")\n",
    "\n",
    "\n",
    "compare_baseline_vs_ft(cv_text, jobs, base_model, ft_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "### Qualitative Results\n",
    "\n",
    "- **Baseline SBERT fails to differentiate roles.**  \n",
    "  All five test jobs received very high similarity scores (0.86–0.96), including clearly unrelated roles like *Kindergarten Teacher* and *Frontend Engineer*. This confirms that the baseline model cannot reliably separate relevant from irrelevant matches.\n",
    "\n",
    "- **Fine-tuned SBERT produces a meaningful ranking.**  \n",
    "  It assigns:\n",
    "  - **very high** similarity to the correct role (*Data Analyst: 0.971*)  \n",
    "  - **moderate** similarity to a partially related role (*BI Developer: 0.693*)  \n",
    "  - **mid-level** similarity to loosely related roles (*ML Engineer: 0.391*)  \n",
    "  - **very low** similarity to unrelated ones (*Frontend Engineer: 0.064*, *Kindergarten Teacher: 0.010*)  \n",
    "  This matches human intuition and shows that the model learned real CV–job matching semantics.\n",
    "\n",
    "- **Generalises beyond training format.**  \n",
    "  Although trained only on structured job profiles (title + skill list), the fine-tuned model also performed well on **manually written, natural-language job descriptions**, indicating it is not overfitting to the input format.\n",
    "\n",
    "- **Learns true skill–role alignment.**  \n",
    "  The model effectively captures the semantic relationship between:\n",
    "  - a candidate’s **experience + skills**, and  \n",
    "  - a job’s **title + required skills**,  \n",
    "  which is exactly the desired behaviour for a resume–job matching system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "## Overall Conclusion\n",
    "- The fine-tuning process successfully **reshaped the embedding space**, creating a clear separation between matching and non-matching CV–job pairs. This was the main objective to improve classification performance for a job recommendation system.\n",
    "\n",
    "- Data cleaning helped reduce noise, but the **true performance gains** came from supervised contrastive training on labelled pairs. The model learned deeper semantic relationships, not just surface-level keyword or skill matching.\n",
    "\n",
    "- The strong separation margin, with negative pairs pushed close to zero, demonstrates that the fine-tuned model now produces **far more meaningful and discriminative similarity scores**, making it highly effective for classification.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7685756,
     "datasetId": 4418374,
     "isSourceIdPinned": false,
     "sourceId": 7590738,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10162530,
     "datasetId": 6085401,
     "sourceId": 9905402,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
